---
content_type: page
description: This section includes learning objective and lecture notes for the course.
learning_resource_types:
- Lecture Notes
ocw_type: CourseSection
title: Lecture Notes
uid: 8f6c4243-744a-9b41-6623-a021d0178db2
---

{{< tableopen >}}
{{< theadopen >}}
{{< tropen >}}
{{< thopen >}}
Lec #
{{< thclose >}}
{{< thopen >}}
Learning Objectives
{{< thclose >}}
{{< thopen >}}
Lecture Notes
{{< thclose >}}

{{< trclose >}}

{{< theadclose >}}
{{< tropen >}}
{{< tdopen >}}
1
{{< tdclose >}}
{{< tdopen >}}


*   To understand how the timescale of diffusion relates to length scales
*   To understand how concentration gradients lead to currents (Fick’s First Law)
*   To understand how charge drift in an electric field leads to currents (Ohm’s Law and resistivity)


{{< tdclose >}}
{{< tdopen >}}
{{% resource_link 6b1b693b-9137-8b6f-a15d-a16eee7621d4 "Overview and Ionic Currents (PDF - 1.7MB)" %}}
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
2
{{< tdclose >}}
{{< tdopen >}}


*   To understand how neurons respond to injected currents
*   To understand how membrane capacitance and resistance allows neurons to integrate or smooth their inputs over time (RC model)
*   To understand how to derive the differential equations for the RC model
*   To be able to sketch the response of an RC neuron to different current inputs
*   To understand where the ‘batteries’ of a neuron come from


{{< tdclose >}}
{{< tdopen >}}
{{% resource_link 45909c8b-f4cb-591d-3f83-95f0ec014fb1 "RC Circuit and Nernst Potential (PDF - 2.7MB)" %}}
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
3
{{< tdclose >}}
{{< tdopen >}}


*   To be able to construct a simplified model neuron by replacing the complex spike generating mechanisms of the real neuron (HH model) with a simplified spike generating mechanism
*   To understand the processes that neurons spend most of their time doing which is integrating inputs in the interval between spikes
*   To be able to create a quantitative description of the firing rate of neurons in response to current inputs
*   To provide an easy-to implement model that captures the basic properties of spiking neurons


{{< tdclose >}}
{{< tdopen >}}
{{% resource_link 16084675-fd6a-490a-2ac0-d3d6ae350072 "Nernst Potential and Integrate and Fire Models​ (PDF - 4.1MB)" %}}
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
4
{{< tdclose >}}
{{< tdopen rowspan="2" >}}


*   To be able to draw the circuit diagram of the HH model
*   Understand what a voltage clamp is and how it works
*   Be able to plot the voltage and time dependence of the potassium current and conductance
*   Be able to explain the time and voltage dependence of the potassium conductance in terms of Hodgkin-Huxley gating variables


{{< tdclose >}}
{{< tdopen >}}
{{% resource_link 4458dad3-0259-d17a-3f1e-df9d7f803fdf "Hodgkin Huxley Model Part 1 (PDF - 6.3MB)" %}}
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
5
{{< tdclose >}}
{{< tdopen >}}
{{% resource_link aaa92d26-e832-b661-a850-e02fbf3bd9bc "Hodgkin Huxley Model Part 2 (PDF - 3.3MB)" %}}
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
6
{{< tdclose >}}
{{< tdopen >}}


*   To be able to draw the ‘circuit diagram’ of a dendrite
*   Be able to plot the voltage in a dendrite as a function of distance for leaky and non-leaky dendrite, and understand the concept of a length constant
*   Know how length constant depends on dendritic radius
*   Understand the concept of electrotonic length
*   Be able to draw the circuit diagram a two-compartment model


{{< tdclose >}}
{{< tdopen >}}
{{% resource_link 65ad58a9-a37d-79e8-6320-8745f30c264d "Dendrites (PDF - 3.2MB)" %}}
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
7
{{< tdclose >}}
{{< tdopen >}}


*   Be able to add a synapse in an equivalent circuit model
*   To describe a simple model of synaptic transmission
*   To be able to describe synaptic transmission as a convolution of a linear kernel with a spike train
*   To understand synaptic saturation
*   To understand the different functions of somatic and dendritic inhibition


{{< tdclose >}}
{{< tdopen >}}
{{% resource_link fff03c31-25fe-8d59-7cec-46ba356862ae "Synapses (PDF - 3.1MB)" %}}
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
8
{{< tdclose >}}
{{< tdopen >}}


*   To understand the origin of extracellular spike waveforms and local field potentials
*   To understand how to extract local field potentials and spike signals by low-pass and high-pass filtering, respectively
*   To be able to extract spike times as a threshold crossing
*   To understand what a peri-stimulus time histogram (PSTH) and a tuning curve is
*   To know how to compute the firing rate of a neuron by smoothing a spike train


{{< tdclose >}}
{{< tdopen >}}
{{% resource_link 7a90ad5e-cc70-3750-6a34-cffd3533935d "Spike Trains (PDF - 2.6MB)" %}}
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
9
{{< tdclose >}}
{{< tdopen >}}


*   To be able to mathematically describe a neural response as a linear filter followed by a nonlinear function.
    *   A correlation of a spatial receptive field with the stimulus
    *   A convolution of a temporal receptive field with the stimulus
*   To understand the concept of a Spatio-temporal Receptive Field (STRF) and the concept of ‘separability’
*   To understand the idea of a Spike Triggered Average and how to use it to compute a Spatio-temporal Receptive Field and a Spectro-temporal Receptive Field (STRF).


{{< tdclose >}}
{{< tdopen >}}
{{% resource_link 2b84cf13-2d42-9d08-9f1f-2709f800a9cd "Receptive Fields (PDF - 2.1MB)" %}}
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
10
{{< tdclose >}}
{{< tdopen >}}


*   Spike trains are probabilistic (Poisson Process)
*   Be able to use measures of spike train variability
    *   Fano Factor
    *   Interspike Interval (ISI)
*   Understand convolution, cross-correlation, and autocorrelation functions
*   Understand the concept of a Fourier series


{{< tdclose >}}
{{< tdopen >}}
{{% resource_link 1bf2762c-9ef0-2991-5015-b61f5eaab59a "Time Series (PDF - 4.5MB)" %}}
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
11
{{< tdclose >}}
{{< tdopen >}}


*   Fourier series for symmetric and asymmetric functions
*   Complex Fourier series
*   Fourier transform
*   Discrete Fourier transform (Fast Fourier Transform - FFT)
*   Power spectrum


{{< tdclose >}}
{{< tdopen >}}
{{% resource_link 53731e24-a7b2-be94-c428-9ba479b11079 "Spectral Analysis Part 1 (PDF - 4.3MB)" %}}
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
12
{{< tdclose >}}
{{< tdopen >}}


*   Fourier Transform Pairs
*   Convolution Theorem
*   Gaussian Noise (Fourier Transform and Power Spectrum)
*   Spectral Estimation
    *   Filtering in the frequency domain
    *   Wiener-Kinchine Theorem
*   Shannon-Nyquist Theorem (and zero padding)
*   Line noise removal


{{< tdclose >}}
{{< tdopen >}}
{{% resource_link 3af4f6a0-ae1d-f1d8-8629-3979018142b0 "Spectral Analysis Part 2 (PDF - 3.1MB)" %}}
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
13
{{< tdclose >}}
{{< tdopen >}}


*   Brief review of Fourier transform pairs and convolution theorem
*   Spectral estimation
    *   Windows and Tapers
*   Spectrograms
*   Multi-taper spectral analysis
    *   How to design the best tapers (DPSS)
    *   Controlling the time-bandwith product
*   Advanced filtering methods


{{< tdclose >}}
{{< tdopen >}}
{{% resource_link 8ff190d8-31c8-c04e-5850-37c28f1e1493 "Spectral Analysis Part 3 (PDF - 2.2MB)" %}}
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
14
{{< tdclose >}}
{{< tdopen >}}


*   Derive a mathematically tractable model of neural networks (the rate model)
*   Building receptive fields with neural networks
*   Vector notation and vector algebra
*   Neural networks for classification
*   Perceptrons


{{< tdclose >}}
{{< tdopen >}}
{{% resource_link 0b7de8f4-cb62-9d99-607a-3179af8a8363 "Rate Models and Perceptrons (PDF - 3.9MB)" %}}
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
15
{{< tdclose >}}
{{< tdopen >}}


*   Perceptrons and perceptron learning rule
*   Neuronal logic, linear separability, and invariance
*   Two-layer feedforward networks
*   Matrix algebra review
*   Matrix transformations


{{< tdclose >}}
{{< tdopen >}}
{{% resource_link 0993f9f8-231d-ae91-8e3a-6d291b13281f "Matrix Operations (PDF - 4.0MB)" %}}
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
16
{{< tdclose >}}
{{< tdopen >}}


*   More on two-layer feed-forward networks
*   Matrix transformations (rotated transformations)
*   Basis sets
*   Linear independence
*   Change of basis


{{< tdclose >}}
{{< tdopen >}}
{{% resource_link a6e8bfd0-a1d7-7c92-718a-3370581f9490 "Basis Sets (PDF - 2.8MB)" %}}
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
17
{{< tdclose >}}
{{< tdopen >}}


*   Eigenvectors and eigenvalues
*   Variance and multivariate Gaussian distributions
*   Computing a covariance matrix from data
*   Principal Components Analysis (PCA)


{{< tdclose >}}
{{< tdopen >}}
{{% resource_link 23203cb4-7ede-79bf-c5c8-c6b1ae2774f2 "Principal Components Analysis​ (PDF - 4.8MB)" %}}
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
18
{{< tdclose >}}
{{< tdopen >}}


*   Mathematical description of recurrent networks
*   Dynamics in simple autapse networks
*   Dynamics in fully recurrent networks
*   Recurrent networks for storing memories
*   Recurrent networks for decision making (winner-take-all)


{{< tdclose >}}
{{< tdopen >}}
{{% resource_link 527a5c41-f04e-5789-82b3-af27c85c2824 "Recurrent Networks (PDF - 2.2MB)" %}}
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
19
{{< tdclose >}}
{{< tdopen >}}


*   Recurrent neural networks and memory
*   The oculomotor system as a model of short term memory and neural integration
*   Stability in neural integrators
*   Learning in neural integrators


{{< tdclose >}}
{{< tdopen >}}
{{% resource_link cd8480fd-34b8-1875-c5d4-e9c808020203 "Neural Integrators (PDF - 2.0MB)" %}}
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
20
{{< tdclose >}}
{{< tdopen >}}


*   Recurrent networks with lambda greater than one
    *   Attractors
*   Winner-take-all networks
*   Attractor networks for long-term memory (Hopfield model)
*   Energy landscape
*   Hopfield network capacity


{{< tdclose >}}
{{< tdopen >}}
{{% resource_link d0eb7c14-d2fa-8ddb-6bed-2583d2f2962d "Hopfield Networks (PDF - 2.7MB)" %}}
{{< tdclose >}}

{{< trclose >}}

{{< tableclose >}}